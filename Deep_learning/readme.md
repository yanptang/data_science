# 深度学习简介

深度学习是机器学习的一个分支，通常指基于人工神经网络的学习方法。它通过多层网络结构自动学习数据的层级特征表示，广泛应用于视觉、语音、自然语言处理等领域。

常见的深度学习模型有：
- **CNN（卷积神经网络）**：善于提取局部空间特征，常用于图像处理与计算机视觉任务。  
- **RNN（循环神经网络）**：用于处理序列数据，适合时间序列和自然语言等任务（包括 LSTM、GRU 等变种）。  
- **Transformer**：基于自注意力机制，能够并行建模长距离依赖，是当前大型语言模型（LLM）的核心架构。

可根据任务特点选择合适的模型或将多种模型组合以提升性能。

```python
#神经网络的历史
感知机（1957）
    ↓ 不能学 XOR
多层感知机（1980s）
    ↓ Step 函数不可导
Sigmoid / ReLU 出现
    ↓ 多层参数复杂
出现 Loss Function + Gradient Descent
    ↓ 多层梯度计算困难
反向传播算法 BP（1986）
    ↓ 数据维度大
CNN（1998）
    ↓ 序列问题
RNN / LSTM（1997, 2014）
    ↓ 长距依赖 & 并行低
Transformer（2017）

#深度学习模型分类
深度学习模型
│
├── 前馈网络（MLP, AE, VAE）
│
├── 卷积网络（CNN）
│
├── 序列模型
│      ├── RNN/LSTM
│      └── Transformer（主流）
│
├── 图神经网络（GNN）
│
├── 生成模型
│      ├── 自回归（GPT）
│      ├── GAN（StyleGAN）
│      └── Diffusion（Stable Diffusion）
│
├── 深度强化学习（DRL）
│
└── 多模态模型（CLIP/LLM-V）

```