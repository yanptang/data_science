{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02411509",
   "metadata": {},
   "source": [
    "在这一小节你将学到：\n",
    "1. 多层感知机MLP是什么\n",
    "2. MLP的隐藏层hidden layer是什么，什么是激活函数\n",
    "- 隐藏层位于输入层和输出层之间，不直接与外界交互，是使用激活函数的地方\n",
    "- 激活函数是引入非线性能力的函数，有了激活函数 → 网络可以近似任何函数（万能逼近定理）\n",
    "\n",
    "3. 如何训练一个多层感知机(简单实例)\n",
    "- 前向传播Forward是什么\n",
    "- 损失函数/代价函数 LossFunction (Cost Function)\n",
    "- backward反向传播算法\n",
    "- 随机梯度下降SGD\n",
    "- 学习率\n",
    "\n",
    "4. 如何调用Pytorch快速实现神经网络并完成训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32c4c7",
   "metadata": {},
   "source": [
    "# 2.多层感知机MLP\n",
    "## 2.1 激活函数\n",
    "如第一小节小结所提，多层感知机能处理非线性的能力，来自于隐藏层的激活函数。我们先通过一些例子来理解最简单，最常用的两个激活函数：Sigmoid，ReLU；从而理解为什么线性的输入层，经过了激活函数，就有了表示非线性的能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad70253",
   "metadata": {},
   "source": [
    "## 2.2 MLP怎么构建呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f588803",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69c99445",
   "metadata": {},
   "source": [
    "## 2.3 MLP的训练过程⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c19e7e00",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31cc4cc4",
   "metadata": {},
   "source": [
    "## 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15f34d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "601bffe7",
   "metadata": {},
   "source": [
    "| 步骤    | 输入            | 核心动作        | 产出             |\n",
    "| ----- | ------------- | ----------- | -------------- |\n",
    "| 1. 前向 | 数据 + 参数       | 矩阵运算 + 激活函数 | 预测结果           |\n",
    "| 2. 损失 | 预测结果 + 真值     | 计算误差函数      | Loss值          |\n",
    "| 3. 反向 | Loss值 + 链式法则  | 求导（找责任）     | 梯度 (Gradient)  |\n",
    "| 4. 更新 | 参数 + 梯度 + 学习率 | 梯度下降（减法）    | 新参数 (New w, b) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544661a",
   "metadata": {},
   "source": [
    "## 附录内容\n",
    "### 其他优化算法（近似最小损失函数）\n",
    "\n",
    "- **Mini-batch SGD**  \n",
    "目前最常用的方法。每次处理一小部分数据（如 1024 个样本，称为一个 Batch），计算平均梯度后更新参数。这种方式兼顾了速度和矩阵运算的效率。\n",
    "\n",
    "- **Adam 优化器**  \n",
    "目前 90% 以上的大模型（如 GPT 系列、Llama、DeepSeek）训练时都采用 Adam（Adaptive Moment Estimation）。它为梯度下降增加了两个“buff”：\n",
    "\n",
    "    - **动量（Momentum）**：不仅考虑当前的梯度，还参考之前的惯性。直观理解：如果之前一直往南走，现在的坡度也向南，就加速冲下去；如果坡度突然变化，也不会立刻掉头，而是受惯性影响平滑转弯。这有助于跨越一些微小的“局部最小值”。\n",
    "    - **自适应学习率（Adaptive Learning Rate）**：为每个参数（如几千亿个 $w$）配备独立的“变速箱”。直观理解：更新频繁、梯度大的参数，学习率调小，避免步子太大；更新慢、梯度小的参数，学习率调大，提高效率。\n",
    "\n",
    "这些优化算法让模型训练更加高效、稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ea3f23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
