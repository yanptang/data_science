# 传统机器学习简介
传统机器学习主要基于几何、概率和统计学，这一概念主要与深度学习（基于多层神经网络）的机器学习模型区分开来。传统的机器学习模型最核心的内容是特征工程（Feature Engineering）。即需要人工处理和提取特征。基本分为两个类别。
**值得强调的是.即使在大模型横行的今天，传统机器学习仍然在工业界具有一席之地。不仅是因为这类模型在需要的资源相对神经网络较少的前提下，也能实现较好的准确率/可用性，最重要的是，它们普遍具有可解释性。
部分工业界并不追求极致的性能和效率，相对百分之几的提升来说，像是银行贷款审核、医疗诊断等容错率极低的领域，可解释性更加重要**

```python
# 传统机器学习的历史
最小二乘法（1805）- 数学起源
    ↓ 解决简单的回归预测
线性回归 (Linear Regression)
    ↓ 数据维度太高，计算太慢
PCA 主成分分析 (1901, 1933)
    ↓ 需要处理“是或否”的分类问题
逻辑回归 (Logistic Regression, 1958)
    ↓ 处理没有标签的数据
K-Means 聚类 (1967)
    ↓ 追求数学上的最优分类间隔
支持向量机 (SVM, 1990s)
    ↓ 统计学无法处理复杂的非线性边界
决策树 (Decision Tree, 1960s-1980s)
    ↓ 单棵树容易“钻牛角尖”（过拟合）
集成学习 (Random Forest, 2001)
    ↓ 追求极致的工业预测精度
梯度提升树 (XGBoost / LightGBM, 2014)


传统机器学习模型
│
├── 监督学习（有标准答案）
│   ├── 回归问题（预测连续数值）
│   │   ├── 线性回归 (Linear Regression)
│   │   └── 岭回归/Lasso (正则化防止过拟合)
│   │
│   └── 分类问题（预测类别）
│       ├── 逻辑回归 (Logistic Regression)
│       ├── softmax回归(多分类问题)
│       ├── 支持向量机 (SVM)
│       ├── 决策树(decision tree)
│       ├── 朴素贝叶斯 (Naive Bayes)
│       └── K-近邻 (KNN)
│
├── 无监督学习（找内在规律）
│   ├── 聚类 (Clustering)
│   │   ├── K-Means
│   │   └── 层级聚类 (Hierarchical)
│   │
│   └── 降维 (Dimensionality Reduction)
│       ├── PCA (主成分分析)
│       └── t-SNE (可视化常用)
│
├── 集成学习（人多力量大）
│   ├── Bagging Aggregating
│   │   └── 随机森林 (Random Forest)
│   │
│   └── Boosting Aggregating
│       ├── AdaBoost
│       ├── XGBoost
│       └── LightGBM
│
└── 关联规则（买面包的人也买牛奶）
│   └── Apriori 算法
│
└── 强化学习
    └── Q-learning

```